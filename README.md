<link rel="stylesheet" type="text/css" href="https://github.com/rhernandez00/Visual-categories-in-dogs-and-humans/blob/main/style.css">

# Visual-categories-in-dogs-and-humans

This repository contains some of the code I wrote related to my upcoming paper in which we compare the visual cortex of humans and dogs using fMRI.

We acquired data from 15 family dogs of different breeds and 13 humans. All procedures involving dogs met national and international guidelines for animal care and were approved by the National Animal Experimentation Ethics Committee. All procedures involving humans were approved by the appropriate ethics committee as well.

In this project I collected short videos from youtube (available under a Creative Commons License). The videos displayed various situations and backgrounds using natural colors to maximize ecological validity. The videos were divided into two superordinate categories (Figure 1), Living and Non-living. The living category was composed of three categories: Dogs, Humans, and Cats. The Non-living superordinate category was represented by a single category: Cars. The videos from the living superordinate category could also show faces or only body parts. My goal with this design was to be able to analyze different encoding levels of the visual system using a single database.

![Figure 1](https://github.com/rhernandez00/Visual-categories-in-dogs-and-humans/blob/main/Figure1.png)
<p class="caption"> Figure 1. Design of a single run. Each run was composed of videos of different categories (cats, dogs, humans and cars), with 12 videos for each category. The videos were separated by a random interstimulus interval (grey screen), every run ended with an additional grey screen. A total of 288 videos were broken into six sets of videos (a single set was played for each run); a video set was composed of 48 videos, each participant experienced once each set, humans and dogs experienced the same sets. The video order was randomized for every run and every participant. </p>